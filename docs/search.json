[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "PRACTICAS",
    "section": "",
    "text": "Code\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom sklearn.model_selection import train_test_split, learning_curve\nfrom sklearn.linear_model import LinearRegression, LogisticRegression\nfrom sklearn.metrics import mean_squared_error, r2_score\n\n\n\n\nCode\n# Lee el CSV que está en la misma carpeta\ndf_reducido = pd.read_csv(\"data/mi_base1.csv\",\nsep=';',\n        encoding='latin-1')\n# Muestra las primeras filas\ndf_reducido.head()\n\n\n\n\n\n\n\n\n\nprov\nsexo\ndcronica\n\n\n\n\n0\n1\n1\n0\n\n\n1\n1\n1\n0\n\n\n2\n1\n1\n1\n\n\n3\n1\n1\n0\n\n\n4\n1\n0\n1\n\n\n\n\n\n\n\n\n\nCode\ndf_reducido.head()\n\n\n\n\n\n\n\n\n\nprov\nsexo\ndcronica\n\n\n\n\n0\n1\n1\n0\n\n\n1\n1\n1\n0\n\n\n2\n1\n1\n1\n\n\n3\n1\n1\n0\n\n\n4\n1\n0\n1\n\n\n\n\n\n\n\n\n\nCode\nX = df_reducido.drop(columns=[\"dcronica\"])\ny = df_reducido[\"dcronica\"]\n\nprint(X)\nprint(y)\n\n\n       prov  sexo\n0         1     1\n1         1     1\n2         1     1\n3         1     1\n4         1     0\n...     ...   ...\n19269    90     0\n19270    90     1\n19271    90     0\n19272    90     0\n19273    90     1\n\n[19274 rows x 2 columns]\n0        0\n1        0\n2        1\n3        0\n4        1\n        ..\n19269    0\n19270    0\n19271    0\n19272    0\n19273    0\nName: dcronica, Length: 19274, dtype: int64\n\n\n\n\n\n\nCode\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=42\n)\n\n\n\n\n\n\n\nCode\nmodel = LogisticRegression()\nmodel.fit(X_train, y_train)\n\n\nLogisticRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LogisticRegression?Documentation for LogisticRegressioniFitted\n        \n            \n                Parameters\n                \n\n\n\n\npenalty \n'l2'\n\n\n\ndual \nFalse\n\n\n\ntol \n0.0001\n\n\n\nC \n1.0\n\n\n\nfit_intercept \nTrue\n\n\n\nintercept_scaling \n1\n\n\n\nclass_weight \nNone\n\n\n\nrandom_state \nNone\n\n\n\nsolver \n'lbfgs'\n\n\n\nmax_iter \n100\n\n\n\nmulti_class \n'deprecated'\n\n\n\nverbose \n0\n\n\n\nwarm_start \nFalse\n\n\n\nn_jobs \nNone\n\n\n\nl1_ratio \nNone"
  },
  {
    "objectID": "index.html#subtitulo-2",
    "href": "index.html#subtitulo-2",
    "title": "PRACTICAS",
    "section": "",
    "text": "Code\n%pip install matplotlib seaborn scikit-learn pandas numpy\n\n\nRequirement already satisfied: matplotlib in f:\\2025\\mast_ia\\1.- fundamentos ia\\.venv\\lib\\site-packages (3.10.6)\nRequirement already satisfied: seaborn in f:\\2025\\mast_ia\\1.- fundamentos ia\\.venv\\lib\\site-packages (0.13.2)\nRequirement already satisfied: scikit-learn in f:\\2025\\mast_ia\\1.- fundamentos ia\\.venv\\lib\\site-packages (1.7.2)\nRequirement already satisfied: pandas in f:\\2025\\mast_ia\\1.- fundamentos ia\\.venv\\lib\\site-packages (2.3.2)\nRequirement already satisfied: numpy in f:\\2025\\mast_ia\\1.- fundamentos ia\\.venv\\lib\\site-packages (2.2.6)\nRequirement already satisfied: contourpy&gt;=1.0.1 in f:\\2025\\mast_ia\\1.- fundamentos ia\\.venv\\lib\\site-packages (from matplotlib) (1.3.3)\nRequirement already satisfied: cycler&gt;=0.10 in f:\\2025\\mast_ia\\1.- fundamentos ia\\.venv\\lib\\site-packages (from matplotlib) (0.12.1)\nRequirement already satisfied: fonttools&gt;=4.22.0 in f:\\2025\\mast_ia\\1.- fundamentos ia\\.venv\\lib\\site-packages (from matplotlib) (4.60.1)\nRequirement already satisfied: kiwisolver&gt;=1.3.1 in f:\\2025\\mast_ia\\1.- fundamentos ia\\.venv\\lib\\site-packages (from matplotlib) (1.4.9)\nRequirement already satisfied: packaging&gt;=20.0 in f:\\2025\\mast_ia\\1.- fundamentos ia\\.venv\\lib\\site-packages (from matplotlib) (25.0)\nRequirement already satisfied: pillow&gt;=8 in f:\\2025\\mast_ia\\1.- fundamentos ia\\.venv\\lib\\site-packages (from matplotlib) (11.3.0)\nRequirement already satisfied: pyparsing&gt;=2.3.1 in f:\\2025\\mast_ia\\1.- fundamentos ia\\.venv\\lib\\site-packages (from matplotlib) (3.2.5)\nRequirement already satisfied: python-dateutil&gt;=2.7 in f:\\2025\\mast_ia\\1.- fundamentos ia\\.venv\\lib\\site-packages (from matplotlib) (2.9.0.post0)\nRequirement already satisfied: scipy&gt;=1.8.0 in f:\\2025\\mast_ia\\1.- fundamentos ia\\.venv\\lib\\site-packages (from scikit-learn) (1.16.2)\nRequirement already satisfied: joblib&gt;=1.2.0 in f:\\2025\\mast_ia\\1.- fundamentos ia\\.venv\\lib\\site-packages (from scikit-learn) (1.5.2)\nRequirement already satisfied: threadpoolctl&gt;=3.1.0 in f:\\2025\\mast_ia\\1.- fundamentos ia\\.venv\\lib\\site-packages (from scikit-learn) (3.6.0)\nRequirement already satisfied: pytz&gt;=2020.1 in f:\\2025\\mast_ia\\1.- fundamentos ia\\.venv\\lib\\site-packages (from pandas) (2025.2)\nRequirement already satisfied: tzdata&gt;=2022.7 in f:\\2025\\mast_ia\\1.- fundamentos ia\\.venv\\lib\\site-packages (from pandas) (2025.2)\nRequirement already satisfied: six&gt;=1.5 in f:\\2025\\mast_ia\\1.- fundamentos ia\\.venv\\lib\\site-packages (from python-dateutil&gt;=2.7-&gt;matplotlib) (1.17.0)\nNote: you may need to restart the kernel to use updated packages.\n\n\n\n\nCode\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import make_classification\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import confusion_matrix, classification_report, roc_auc_score, roc_curve\nimport seaborn as sns"
  },
  {
    "objectID": "index.html#generar-y-cargar-datos",
    "href": "index.html#generar-y-cargar-datos",
    "title": "PRACTICAS",
    "section": "",
    "text": "Code\nfrom sklearn.datasets import make_classification\n\n# Generar dataset ficticio\nX, y = make_classification(n_samples=1000, n_features=5, \n                           n_informative=3, n_redundant=0, \n                           random_state=42)\n\n# Dividir en entrenamiento y prueba\nX_train, X_test, y_train, y_test = train_test_split(X, y, \n                                                    test_size=0.3, \n                                                    random_state=42)\n\n\n\n\n\n\nCode\n# Crear el modelo\nmodelo = LogisticRegression()\n\n# Entrenar con los datos\nmodelo.fit(X_train, y_train)\n\n\nLogisticRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LogisticRegression?Documentation for LogisticRegressioniFitted\n        \n            \n                Parameters\n                \n\n\n\n\npenalty \n'l2'\n\n\n\ndual \nFalse\n\n\n\ntol \n0.0001\n\n\n\nC \n1.0\n\n\n\nfit_intercept \nTrue\n\n\n\nintercept_scaling \n1\n\n\n\nclass_weight \nNone\n\n\n\nrandom_state \nNone\n\n\n\nsolver \n'lbfgs'\n\n\n\nmax_iter \n100\n\n\n\nmulti_class \n'deprecated'\n\n\n\nverbose \n0\n\n\n\nwarm_start \nFalse\n\n\n\nn_jobs \nNone\n\n\n\nl1_ratio \nNone\n\n\n\n\n            \n        \n    \n\n\n\n\n\n\n\nCode\n# Predicción de clases\ny_pred = modelo.predict(X_test)\n\n# Predicción de probabilidades\ny_prob = modelo.predict_proba(X_test)[:,1]  # prob. de clase positiva\n\n\n\n\n\n\n\nCode\n# Matriz de confusión\nprint(\"Matriz de Confusión:\")\nprint(confusion_matrix(y_test, y_pred))\n\n# Reporte de métricas\nprint(\"\\nReporte de Clasificación:\")\nprint(classification_report(y_test, y_pred))\n\n# AUC - ROC\nauc = roc_auc_score(y_test, y_prob)\nprint(f\"AUC: {auc:.3f}\")\n\n\nMatriz de Confusión:\n[[127  11]\n [ 17 145]]\n\nReporte de Clasificación:\n              precision    recall  f1-score   support\n\n           0       0.88      0.92      0.90       138\n           1       0.93      0.90      0.91       162\n\n    accuracy                           0.91       300\n   macro avg       0.91      0.91      0.91       300\nweighted avg       0.91      0.91      0.91       300\n\nAUC: 0.975\n\n\n\n\n\n\n\nCode\nfpr, tpr, thresholds = roc_curve(y_test, y_prob)\nplt.plot(fpr, tpr, label=f'AUC={auc:.2f}')\nplt.plot([0,1],[0,1],'--')\nplt.xlabel(\"False Positive Rate\")\nplt.ylabel(\"True Positive Rate\")\nplt.title(\"Curva ROC\")\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\ncm = confusion_matrix(y_test, y_pred)\nsns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\",\n            xticklabels=[\"Clase 0\",\"Clase 1\"],\n            yticklabels=[\"Clase 0\",\"Clase 1\"])\nplt.title(\"Matriz de Confusión\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nfrom sklearn.datasets import load_breast_cancer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, ConfusionMatrixDisplay\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\n\n\n\n\n\n\n\nCode\ndata = load_breast_cancer()\n#data\nX = data.data #columnas\ny = data.target #filas\n\n\n\n\n\n\n\nCode\nX_train, X_test, y_train, y_test = train_test_split(\n    X,y,test_size=0.2,random_state=42,stratify=y\n)\n\nprint(X_train)\nprint(X_test)\n\n\n[[1.032e+01 1.635e+01 6.531e+01 ... 2.381e-02 2.681e-01 7.399e-02]\n [2.018e+01 1.954e+01 1.338e+02 ... 2.173e-01 3.032e-01 8.075e-02]\n [1.066e+01 1.515e+01 6.749e+01 ... 0.000e+00 2.710e-01 6.164e-02]\n ...\n [1.546e+01 2.395e+01 1.038e+02 ... 2.163e-01 3.013e-01 1.067e-01]\n [1.705e+01 1.908e+01 1.134e+02 ... 2.543e-01 3.109e-01 9.061e-02]\n [1.088e+01 1.562e+01 7.041e+01 ... 7.966e-02 2.581e-01 1.080e-01]]\n[[1.955e+01 2.877e+01 1.336e+02 ... 1.941e-01 2.818e-01 1.005e-01]\n [1.113e+01 1.662e+01 7.047e+01 ... 4.044e-02 2.383e-01 7.083e-02]\n [1.382e+01 2.449e+01 9.233e+01 ... 1.521e-01 3.651e-01 1.183e-01]\n ...\n [1.532e+01 1.727e+01 1.032e+02 ... 2.229e-01 3.258e-01 1.191e-01]\n [1.262e+01 2.397e+01 8.135e+01 ... 1.180e-01 2.826e-01 9.585e-02]\n [1.168e+01 1.617e+01 7.549e+01 ... 9.815e-02 2.804e-01 8.024e-02]]\n\n\n\n\n\n\n\nCode\nmodel = LogisticRegression(max_iter=100000)\nmodel.fit(X_train,y_train)\n\n\nLogisticRegression(max_iter=100000)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LogisticRegression?Documentation for LogisticRegressioniFitted\n        \n            \n                Parameters\n                \n\n\n\n\npenalty \n'l2'\n\n\n\ndual \nFalse\n\n\n\ntol \n0.0001\n\n\n\nC \n1.0\n\n\n\nfit_intercept \nTrue\n\n\n\nintercept_scaling \n1\n\n\n\nclass_weight \nNone\n\n\n\nrandom_state \nNone\n\n\n\nsolver \n'lbfgs'\n\n\n\nmax_iter \n100000\n\n\n\nmulti_class \n'deprecated'\n\n\n\nverbose \n0\n\n\n\nwarm_start \nFalse\n\n\n\nn_jobs \nNone\n\n\n\nl1_ratio \nNone\n\n\n\n\n            \n        \n    \n\n\n\n\n\n\n\nCode\ny_pred = model.predict(X_test)\n\n\n\n\n\n\n\nCode\naccurancy = accuracy_score(y_test, y_pred)\nprecision = precision_score(y_test, y_pred)\nrecall =recall_score(y_test, y_pred)\nf1 = f1_score(y_test, y_pred)\n\nprint(\"accurancy =\", accurancy)\nprint(\"precision =\",precision)\nprint(\"recall =\", recall)\nprint(\"f1 =\", f1)\n\n\naccurancy = 0.9649122807017544\nprecision = 0.9594594594594594\nrecall = 0.9861111111111112\nf1 = 0.9726027397260274\n\n\n\n\nCode\nConfusionMatrixDisplay.from_predictions(y_test,y_pred)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\ndata = load_breast_cancer()\n#data\nX = data.data #columnas\ny = data.target #filas\n\n\n\n\n\n\n\nCode\nX_train, X_test, y_train, y_test = train_test_split(\n    X,y,test_size=0.2,random_state=42,stratify=y\n)\n\nprint(X_train)\nprint(X_test)\n\n\n[[1.032e+01 1.635e+01 6.531e+01 ... 2.381e-02 2.681e-01 7.399e-02]\n [2.018e+01 1.954e+01 1.338e+02 ... 2.173e-01 3.032e-01 8.075e-02]\n [1.066e+01 1.515e+01 6.749e+01 ... 0.000e+00 2.710e-01 6.164e-02]\n ...\n [1.546e+01 2.395e+01 1.038e+02 ... 2.163e-01 3.013e-01 1.067e-01]\n [1.705e+01 1.908e+01 1.134e+02 ... 2.543e-01 3.109e-01 9.061e-02]\n [1.088e+01 1.562e+01 7.041e+01 ... 7.966e-02 2.581e-01 1.080e-01]]\n[[1.955e+01 2.877e+01 1.336e+02 ... 1.941e-01 2.818e-01 1.005e-01]\n [1.113e+01 1.662e+01 7.047e+01 ... 4.044e-02 2.383e-01 7.083e-02]\n [1.382e+01 2.449e+01 9.233e+01 ... 1.521e-01 3.651e-01 1.183e-01]\n ...\n [1.532e+01 1.727e+01 1.032e+02 ... 2.229e-01 3.258e-01 1.191e-01]\n [1.262e+01 2.397e+01 8.135e+01 ... 1.180e-01 2.826e-01 9.585e-02]\n [1.168e+01 1.617e+01 7.549e+01 ... 9.815e-02 2.804e-01 8.024e-02]]\n\n\n\n\n\n\n\nCode\npipe = Pipeline ([\n    (\"Escalado\", StandardScaler()),\n    (\"LogicR\", LogisticRegression(max_iter=100000))\n])\n\n\n\n\n\n\n\nCode\npipe.fit(X_train, y_train)\n\n\nPipeline(steps=[('Escalado', StandardScaler()),\n                ('LogicR', LogisticRegression(max_iter=100000))])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.Pipeline?Documentation for PipelineiFitted\n        \n            \n                Parameters\n                \n\n\n\n\nsteps \n[('Escalado', ...), ('LogicR', ...)]\n\n\n\ntransform_input \nNone\n\n\n\nmemory \nNone\n\n\n\nverbose \nFalse\n\n\n\n\n            \n        \n    StandardScaler?Documentation for StandardScaler\n        \n            \n                Parameters\n                \n\n\n\n\ncopy \nTrue\n\n\n\nwith_mean \nTrue\n\n\n\nwith_std \nTrue\n\n\n\n\n            \n        \n    LogisticRegression?Documentation for LogisticRegression\n        \n            \n                Parameters\n                \n\n\n\n\npenalty \n'l2'\n\n\n\ndual \nFalse\n\n\n\ntol \n0.0001\n\n\n\nC \n1.0\n\n\n\nfit_intercept \nTrue\n\n\n\nintercept_scaling \n1\n\n\n\nclass_weight \nNone\n\n\n\nrandom_state \nNone\n\n\n\nsolver \n'lbfgs'\n\n\n\nmax_iter \n100000\n\n\n\nmulti_class \n'deprecated'\n\n\n\nverbose \n0\n\n\n\nwarm_start \nFalse\n\n\n\nn_jobs \nNone\n\n\n\nl1_ratio \nNone\n\n\n\n\n            \n        \n    \n\n\n\n\n\n\n\nCode\ny_pred_pipe = pipe.predict(X_test)\n\n\n\n\n\n\n\nCode\naccurancy = accuracy_score(y_test, y_pred_pipe)\nprecision = precision_score(y_test, y_pred_pipe)\nrecall =recall_score(y_test, y_pred_pipe)\nf1 = f1_score(y_test, y_pred_pipe)\n\nprint(\"accurancy =\", accurancy)\nprint(\"precision =\",precision)\nprint(\"recall =\", recall)\nprint(\"f1 =\", f1)\n\n\naccurancy = 0.9824561403508771\nprecision = 0.9861111111111112\nrecall = 0.9861111111111112\nf1 = 0.9861111111111112\n\n\n\n\nCode\nConfusionMatrixDisplay.from_predictions(y_test,y_pred_pipe)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nimport pandas as pd\nimport altair as alt\n\n# Lee el CSV que está en la misma carpeta\ndf = pd.read_csv(\"data/Regr_BDD.csv\",\nsep=';',\n        encoding='latin-1')\n# Muestra las primeras filas\ndf.head()\n\n\n\n\n\n\n\n\n\nï»¿area\nprov\nupm\nid_viv\nid_hogar\nid_per\npersona\nsexo\nf1_s2_3_1\nf1_s2_3_2\n...\ndobes12_19\ndspob12_19\nddelg12_19\ndbpeso19_59\ndnorm19_59\ndspeso19_59\ndobes19_59\ndspobes19_59\nfexp\nestrato\n\n\n\n\n0\nurbano\n1\n10150000201\n1015000020101\n10150000201011\n1015000020101101\n1\nhombre\n28\n\n...\n\n\n\n0\n0\n1\n0\n1\n43,406982421875\n2713\n\n\n1\nurbano\n1\n10150000201\n1015000020101\n10150000201011\n1015000020101102\n2\nmujer\n28\n\n...\n\n\n\n\n\n\n\n\n42,165885925293\n2713\n\n\n2\nurbano\n1\n10150000201\n1015000020101\n10150000201011\n1015000020101103\n3\nhombre\n13\n\n...\n0\n1\n0\n\n\n\n\n\n43,406982421875\n2713\n\n\n3\nurbano\n1\n10150000201\n1015000020101\n10150000201011\n1015000020101104\n4\nhombre\n11\n\n...\n\n\n\n\n\n\n\n\n36,9120407104492\n2713\n\n\n4\nurbano\n1\n10150000201\n1015000020101\n10150000201011\n1015000020101105\n5\nhombre\n6\n\n...\n\n\n\n\n\n\n\n\n36,9120407104492\n2713\n\n\n\n\n5 rows × 264 columns"
  },
  {
    "objectID": "index.html#se-escogio-una-data",
    "href": "index.html#se-escogio-una-data",
    "title": "PRACTICAS",
    "section": "",
    "text": "Code\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom sklearn.model_selection import train_test_split, learning_curve\nfrom sklearn.linear_model import LinearRegression, LogisticRegression\nfrom sklearn.metrics import mean_squared_error, r2_score\n\n\n\n\nCode\n# Lee el CSV que está en la misma carpeta\ndf_reducido = pd.read_csv(\"data/mi_base1.csv\",\nsep=';',\n        encoding='latin-1')\n# Muestra las primeras filas\ndf_reducido.head()\n\n\n\n\n\n\n\n\n\nprov\nsexo\ndcronica\n\n\n\n\n0\n1\n1\n0\n\n\n1\n1\n1\n0\n\n\n2\n1\n1\n1\n\n\n3\n1\n1\n0\n\n\n4\n1\n0\n1\n\n\n\n\n\n\n\n\n\nCode\ndf_reducido.head()\n\n\n\n\n\n\n\n\n\nprov\nsexo\ndcronica\n\n\n\n\n0\n1\n1\n0\n\n\n1\n1\n1\n0\n\n\n2\n1\n1\n1\n\n\n3\n1\n1\n0\n\n\n4\n1\n0\n1\n\n\n\n\n\n\n\n\n\nCode\nX = df_reducido.drop(columns=[\"dcronica\"])\ny = df_reducido[\"dcronica\"]\n\nprint(X)\nprint(y)\n\n\n       prov  sexo\n0         1     1\n1         1     1\n2         1     1\n3         1     1\n4         1     0\n...     ...   ...\n19269    90     0\n19270    90     1\n19271    90     0\n19272    90     0\n19273    90     1\n\n[19274 rows x 2 columns]\n0        0\n1        0\n2        1\n3        0\n4        1\n        ..\n19269    0\n19270    0\n19271    0\n19272    0\n19273    0\nName: dcronica, Length: 19274, dtype: int64\n\n\n\n\n\n\nCode\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=42\n)\n\n\n\n\n\n\n\nCode\nmodel = LogisticRegression()\nmodel.fit(X_train, y_train)\n\n\nLogisticRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LogisticRegression?Documentation for LogisticRegressioniFitted\n        \n            \n                Parameters\n                \n\n\n\n\npenalty \n'l2'\n\n\n\ndual \nFalse\n\n\n\ntol \n0.0001\n\n\n\nC \n1.0\n\n\n\nfit_intercept \nTrue\n\n\n\nintercept_scaling \n1\n\n\n\nclass_weight \nNone\n\n\n\nrandom_state \nNone\n\n\n\nsolver \n'lbfgs'\n\n\n\nmax_iter \n100\n\n\n\nmulti_class \n'deprecated'\n\n\n\nverbose \n0\n\n\n\nwarm_start \nFalse\n\n\n\nn_jobs \nNone\n\n\n\nl1_ratio \nNone"
  },
  {
    "objectID": "mdg.html",
    "href": "mdg.html",
    "title": "Categorical",
    "section": "",
    "text": "Code\nimport pandas as pd\nfrom sklearn.preprocessing import LabelEncoder, OrdinalEncoder, OneHotEncoder, StandardScaler\nfrom sklearn.compose import ColumnTransformer\n\n\n\n\nCode\ndata = {\n    'size' : ['M', 'L', 'S'],\n    'color':['green', 'red', 'blue'],\n    'price':[10.1, 13.5, 15.6],\n    'label':['pantalones', 'camiseta', 'camiseta']\n}\n\ndf = pd.DataFrame(data)\ndf\n\n\n\n\n\n\n\n\n\nsize\ncolor\nprice\nlabel\n\n\n\n\n0\nM\ngreen\n10.1\npantalones\n\n\n1\nL\nred\n13.5\ncamiseta\n\n\n2\nS\nblue\n15.6\ncamiseta\n\n\n\n\n\n\n\n\nColumn transformers\n\n\nCode\ncategorical_features = ['color']\nordinal_features = ['size']\nnumerical_features = ['price']\nsize_order = [['S','M','L']]\n\npreprocessor = ColumnTransformer (\n    transformers=[\n        ('color_onehot', OneHotEncoder(),),\n        ('size_ordenc', OrdinalEncoder(categories=size_order), ordinal_features),\n        ('price_stan', StandardScaler(),numerical_features)\n\n    ]\n)\n\n\n\n\nCode\nimport pandas as pd\nfrom sklearn.datasets import fetch_openml\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import OneHotEncoder, OrdinalEncoder, LabelEncoder, StandardScaler\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.metrics import classification_report, ConfusionMatrixDisplay\n\n\n\n\ncargar dataset\n\n\nCode\ndata = fetch_openml('adult', version=2, as_frame=True)\ndf = data.frame.copy()\ndf\n\n\n\n\n\n\n\n\n\nage\nworkclass\nfnlwgt\neducation\neducation-num\nmarital-status\noccupation\nrelationship\nrace\nsex\ncapital-gain\ncapital-loss\nhours-per-week\nnative-country\nclass\n\n\n\n\n0\n25\nPrivate\n226802\n11th\n7\nNever-married\nMachine-op-inspct\nOwn-child\nBlack\nMale\n0\n0\n40\nUnited-States\n&lt;=50K\n\n\n1\n38\nPrivate\n89814\nHS-grad\n9\nMarried-civ-spouse\nFarming-fishing\nHusband\nWhite\nMale\n0\n0\n50\nUnited-States\n&lt;=50K\n\n\n2\n28\nLocal-gov\n336951\nAssoc-acdm\n12\nMarried-civ-spouse\nProtective-serv\nHusband\nWhite\nMale\n0\n0\n40\nUnited-States\n&gt;50K\n\n\n3\n44\nPrivate\n160323\nSome-college\n10\nMarried-civ-spouse\nMachine-op-inspct\nHusband\nBlack\nMale\n7688\n0\n40\nUnited-States\n&gt;50K\n\n\n4\n18\nNaN\n103497\nSome-college\n10\nNever-married\nNaN\nOwn-child\nWhite\nFemale\n0\n0\n30\nUnited-States\n&lt;=50K\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n48837\n27\nPrivate\n257302\nAssoc-acdm\n12\nMarried-civ-spouse\nTech-support\nWife\nWhite\nFemale\n0\n0\n38\nUnited-States\n&lt;=50K\n\n\n48838\n40\nPrivate\n154374\nHS-grad\n9\nMarried-civ-spouse\nMachine-op-inspct\nHusband\nWhite\nMale\n0\n0\n40\nUnited-States\n&gt;50K\n\n\n48839\n58\nPrivate\n151910\nHS-grad\n9\nWidowed\nAdm-clerical\nUnmarried\nWhite\nFemale\n0\n0\n40\nUnited-States\n&lt;=50K\n\n\n48840\n22\nPrivate\n201490\nHS-grad\n9\nNever-married\nAdm-clerical\nOwn-child\nWhite\nMale\n0\n0\n20\nUnited-States\n&lt;=50K\n\n\n48841\n52\nSelf-emp-inc\n287927\nHS-grad\n9\nMarried-civ-spouse\nExec-managerial\nWife\nWhite\nFemale\n15024\n0\n40\nUnited-States\n&gt;50K\n\n\n\n\n48842 rows × 15 columns\n\n\n\n\n\nCode\ndf = df.dropna()\n\n\n\n\nCode\nX = df.drop(\"class\", axis=1)\ny = df[\"class\"]\n\n\n\n\nCode\nX_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2,random_state=42,stratify=y)\n\n\n\n\nCode\n#for i in df['education'].unique():\n #   print(i)\n\neducation_order = [\"Preschool\",\n                    \"1st-4th\",\n                    \"5th-6th\",\n                    \"7th-8th\",\n                    \"9th\",\n                    \"10th\",\n                    \"11th\",\n                    \"12th\",\n                    \"HS-grad\",\n                    \"Some-college\",\n                    \"Assoc-acdm\",\n                    \"Assoc-voc\",\n                    \"Prof-school\",\n                    \"Bachelors\",\n                    \"Masters\",\n                    \"Doctorate\"]\n\n\n\n\nCode\ncategorical_columns = X.select_dtypes(include='category').columns.tolist()\n\ncategorical_columns_nominal = [col for col in categorical_columns if col != 'education']\ncategorical_columns_nominal\n\nnumerical_columns = X.select_dtypes(include='number').columns.tolist()\n\n\n\n\nCode\nprocessor = ColumnTransformer(\n    transformers=[\n        ('num', StandardScaler(), numerical_columns),\n        ('edu', OrdinalEncoder(categories=[education_order]), ['education']),\n        ['onehot', OneHotEncoder(handle_unknown='ignore'), categorical_columns_nominal]\n\n    ]\n)\n\n\n\n\nCode\nlabel_target_encoded = LabelEncoder()\ny_train_encoded = label_target_encoded.fit_transform(y_train)\ny_test_encoded = label_target_encoded.fit_transform(y_test)\n\n\n\n\nCode\n#pipeline\npipeline = Pipeline([\n    ('preprocesamiento', processor),\n    ('classifier', LogisticRegression(max_iter=10000))\n])\n\n\n\n\nCode\npipeline.fit(X_train,y_train_encoded)\n\n\nPipeline(steps=[('preprocesamiento',\n                 ColumnTransformer(transformers=[('num', StandardScaler(),\n                                                  ['age', 'fnlwgt',\n                                                   'education-num',\n                                                   'capital-gain',\n                                                   'capital-loss',\n                                                   'hours-per-week']),\n                                                 ('edu',\n                                                  OrdinalEncoder(categories=[['Preschool',\n                                                                              '1st-4th',\n                                                                              '5th-6th',\n                                                                              '7th-8th',\n                                                                              '9th',\n                                                                              '10th',\n                                                                              '11th',\n                                                                              '12th',\n                                                                              'HS-grad',\n                                                                              'Some-college',\n                                                                              'Assoc-acdm',\n                                                                              'Assoc-voc',\n                                                                              'Prof-school',\n                                                                              'Bachelors',\n                                                                              'Masters',\n                                                                              'Doctorate']]),\n                                                  ['education']),\n                                                 ['onehot',\n                                                  OneHotEncoder(handle_unknown='ignore'),\n                                                  ['workclass',\n                                                   'marital-status',\n                                                   'occupation', 'relationship',\n                                                   'race', 'sex',\n                                                   'native-country']]])),\n                ('classifier', LogisticRegression(max_iter=10000))])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.Pipeline?Documentation for PipelineiFitted\n        \n            \n                Parameters\n                \n\n\n\n\nsteps \n[('preprocesamiento', ...), ('classifier', ...)]\n\n\n\ntransform_input \nNone\n\n\n\nmemory \nNone\n\n\n\nverbose \nFalse\n\n\n\n\n            \n        \n    preprocesamiento: ColumnTransformer?Documentation for preprocesamiento: ColumnTransformer\n        \n            \n                Parameters\n                \n\n\n\n\ntransformers \n[('num', ...), ('edu', ...), ...]\n\n\n\nremainder \n'drop'\n\n\n\nsparse_threshold \n0.3\n\n\n\nn_jobs \nNone\n\n\n\ntransformer_weights \nNone\n\n\n\nverbose \nFalse\n\n\n\nverbose_feature_names_out \nTrue\n\n\n\nforce_int_remainder_cols \n'deprecated'\n\n\n\n\n            \n        \n    num['age', 'fnlwgt', 'education-num', 'capital-gain', 'capital-loss', 'hours-per-week']StandardScaler?Documentation for StandardScaler\n        \n            \n                Parameters\n                \n\n\n\n\ncopy \nTrue\n\n\n\nwith_mean \nTrue\n\n\n\nwith_std \nTrue\n\n\n\n\n            \n        \n    edu['education']OrdinalEncoder?Documentation for OrdinalEncoder\n        \n            \n                Parameters\n                \n\n\n\n\ncategories \n[['Preschool', '1st-4th', ...]]\n\n\n\ndtype \n&lt;class 'numpy.float64'&gt;\n\n\n\nhandle_unknown \n'error'\n\n\n\nunknown_value \nNone\n\n\n\nencoded_missing_value \nnan\n\n\n\nmin_frequency \nNone\n\n\n\nmax_categories \nNone\n\n\n\n\n            \n        \n    onehot['workclass', 'marital-status', 'occupation', 'relationship', 'race', 'sex', 'native-country']OneHotEncoder?Documentation for OneHotEncoder\n        \n            \n                Parameters\n                \n\n\n\n\ncategories \n'auto'\n\n\n\ndrop \nNone\n\n\n\nsparse_output \nTrue\n\n\n\ndtype \n&lt;class 'numpy.float64'&gt;\n\n\n\nhandle_unknown \n'ignore'\n\n\n\nmin_frequency \nNone\n\n\n\nmax_categories \nNone\n\n\n\nfeature_name_combiner \n'concat'\n\n\n\n\n            \n        \n    LogisticRegression?Documentation for LogisticRegression\n        \n            \n                Parameters\n                \n\n\n\n\npenalty \n'l2'\n\n\n\ndual \nFalse\n\n\n\ntol \n0.0001\n\n\n\nC \n1.0\n\n\n\nfit_intercept \nTrue\n\n\n\nintercept_scaling \n1\n\n\n\nclass_weight \nNone\n\n\n\nrandom_state \nNone\n\n\n\nsolver \n'lbfgs'\n\n\n\nmax_iter \n10000\n\n\n\nmulti_class \n'deprecated'\n\n\n\nverbose \n0\n\n\n\nwarm_start \nFalse\n\n\n\nn_jobs \nNone\n\n\n\nl1_ratio \nNone\n\n\n\n\n            \n        \n    \n\n\n\n\nCode\ny_pred = pipeline.predict(X_test)\n\n\n\n\nCode\nprint(classification_report(y_test_encoded,y_pred,target_names=label_target_encoded.classes_))\n\n\n              precision    recall  f1-score   support\n\n       &lt;=50K       0.88      0.93      0.90      6803\n        &gt;50K       0.73      0.60      0.66      2242\n\n    accuracy                           0.85      9045\n   macro avg       0.80      0.76      0.78      9045\nweighted avg       0.84      0.85      0.84      9045\n\n\n\n\n\nCode\nConfusionMatrixDisplay.from_predictions(y_test_encoded,y_pred)"
  }
]